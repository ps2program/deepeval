{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3502c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "802.79s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™Œ Congratulations! You're now using a LiteLLM model for all evals that require \n",
      "an LLM.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-litellm lm_studio/Meta-Llama-3.1-8B-Instruct-GGUF --api-key=\"lm-studio\" --api-base=\"http://localhost:1234/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a97a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "934.57s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™Œ Congratulations! You're now using regular OpenAI for all evals that require \n",
      "an LLM.\n"
     ]
    }
   ],
   "source": [
    "!deepeval unset-litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2967792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/prahlad/Desktop/Projects/deepeval/venv/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/prahlad/Desktop/Projects/deepeval/venv/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pytest\n",
    "from deepeval.models.llms.litellm_model import LiteLLMModel\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "# Initialize the model with LMStudio configuration\n",
    "model = LiteLLMModel(\n",
    "    # model=\"lm_studio/Meta-Llama-3.1-8B-Instruct-GGUF\",  # LMStudio uses OpenAI-compatible API\n",
    "    model=\"lm_studio/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "    api_key=\"lm-studio\",\n",
    "    api_base=\"http://localhost:1234/v1\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "def test_litellm_with_geval_basic(model):\n",
    "    \"\"\"Test basic usage of G-Eval metric with LiteLLM.\"\"\"\n",
    "    # Create a test case\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"What is the capital of France?\",\n",
    "        actual_output=\"The capital of France is Paris.\",\n",
    "        expected_output=\"Paris\",\n",
    "        context=[\"France is a country in Europe. Its capital city is Paris.\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize G-Eval metric\n",
    "    metric = GEval(\n",
    "        name=\"answer-accuracy\",\n",
    "        criteria=\"Evaluate if the answer correctly identifies the capital of France.\",\n",
    "        evaluation_params=[\n",
    "            LLMTestCaseParams.INPUT,\n",
    "            LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "            LLMTestCaseParams.EXPECTED_OUTPUT\n",
    "        ],\n",
    "        model=model,\n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Measure the score\n",
    "    score = metric.measure(test_case)\n",
    "    \n",
    "    # Assertions\n",
    "    assert isinstance(score, float)\n",
    "    assert 0 <= score <= 1\n",
    "    assert score >= 0.5  # Since the answer is correct\n",
    "test_litellm_with_geval_basic(model)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
